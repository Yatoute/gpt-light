{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d8cc05a",
   "metadata": {},
   "source": [
    "## Coder les mécanismes d'attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314eb10e",
   "metadata": {},
   "source": [
    "À ce stade, nous savons préparer le texte d'entrée pour l'entraînement d'un LLM en le découpant en tokens de mots et de sous-mots qui peuvent être encodés en représentations vectorielles, les embeddings, pour le modèle.\n",
    "\n",
    "Nous allons maintenant examiner un élément essentiel de l'architecture d'un LLM : les mécanismes d'attention. Nous les étudierons en grande partie de manière isolée et d'un point de vue mécaniste. Ensuite, nous coderons les autres composants du LLM qui entourent le mécanisme d'auto-attention afin de l'observer en action et de construire un modèle capable de générer du texte.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d977dd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69175a09",
   "metadata": {},
   "source": [
    "### Un simple mécanisme d'auto-attention sans poids entraînables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9ba0d8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Considérons la phrase d'entrée suivante, déjà convertie en vecteurs tridimensionnels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44712002",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "    [\n",
    "        [0.43, 0.15, 0.89], # Your(x^1)\n",
    "        [0.55, 0.87, 0.66], # journey(x^2)\n",
    "        [0.57, 0.85, 0.64], # starts(x^3)\n",
    "        [0.22, 0.58, 0.33], # with(x^4)\n",
    "        [0.77, 0.25, 0.10], # one(x^5)\n",
    "        [0.05, 0.80, 0.55]# step(x^6)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b739fb5",
   "metadata": {},
   "source": [
    "La première étape de l'auto-attention consiste à calculer les valeurs intermédiaires ω, appelées scores d'attention.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242b42da",
   "metadata": {},
   "source": [
    "- Scores d'attention pour le deuxième token de l'entrée : journey(x^2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c8495d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n"
     ]
    }
   ],
   "source": [
    "query2 = inputs[1]\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, key in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(query2, key)\n",
    "\n",
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9017b7",
   "metadata": {},
   "source": [
    "En pratique, il est plus courant et recommandé d'utiliser la fonction softmax pour la normalisation. Cette approche gère mieux les valeurs extrêmes et offre des propriétés de gradient plus favorables pendant l'entraînement. Le code suivant montre une implémentation de base de softmax pour normaliser les scores d'attention :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ae92df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00344d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "print(\"Attention weights:\", attn_weights_2_naive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b937ac12",
   "metadata": {},
   "source": [
    "Notez que cette implémentation naïve de softmax (`softmax_naive`) peut rencontrer des problèmes de stabilité numérique, tels que des dépassements ou sous-dépassements, lorsque les valeurs d'entrée sont très grandes ou très petites. En pratique, il est donc conseillé d'utiliser l'implémentation PyTorch de softmax, fortement optimisée :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "125b69b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(\"Attention weights:\", attn_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e9bf25",
   "metadata": {},
   "source": [
    "Maintenant que nous avons calculé les poids d'attention normalisés, nous pouvons passer à l'étape finale : calculer le vecteur de contexte z(2) en multipliant les tokens d'entrée encodés x(i) par les poids d'attention correspondants puis en sommant les vecteurs obtenus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "468d796b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216660d9",
   "metadata": {},
   "source": [
    "- Calculer les pondérations d'attention pour tous les tokens d'entrée\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a509438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.zeros(inputs.shape[0], inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fb537f",
   "metadata": {},
   "source": [
    "Lors du calcul du tenseur de scores d'attention précédent, nous avons utilisé des boucles `for` en Python. Cependant, ces boucles sont généralement lentes, et nous pouvons obtenir les mêmes résultats via des multiplications de matrices :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "45e8349c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs@inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd100e9",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant normaliser chaque ligne avec softmax de sorte que la somme des valeurs de chaque ligne soit égale à 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f957ac51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4684fa",
   "metadata": {},
   "source": [
    "Dans la troisième et dernière étape, nous utilisons ces poids d'attention pour calculer tous les vecteurs de contexte via une multiplication matricielle :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0e7765cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "context_vecs = attn_weights@inputs\n",
    "print(context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085d004a",
   "metadata": {},
   "source": [
    "### Implémenter l'auto-attention avec des poids entraînables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c13d5c",
   "metadata": {},
   "source": [
    "L'étape suivante consiste à implémenter le mécanisme d'auto-attention utilisé dans l'architecture Transformer originale, les modèles GPT et la plupart des LLM populaires. Ce mécanisme d'auto-attention est également appelé attention par produit scalaire normalisé (« scaled dot-product attention »).\n",
    "\n",
    "La différence la plus notable est l'introduction de matrices de poids mises à jour pendant l'entraînement. Ces matrices entraînables sont cruciales pour que le modèle (et plus précisément le module d'attention) apprenne à produire de « bons » vecteurs de contexte.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b4e87e",
   "metadata": {},
   "source": [
    "- Calculer les poids d'attention étape par étape\n",
    "\n",
    "Nous allons implémenter le mécanisme d'auto-attention pas à pas en introduisant les trois matrices de poids entraînables W_q, W_k et W_v. Ces trois matrices projettent les tokens encodés x(i) vers des vecteurs requête (query), clé (key) et valeur (value), respectivement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e28542",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "185fd61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6ca8f5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2@W_query\n",
    "key_2 = x_2@W_key\n",
    "value_2 = x_2@W_value\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc660662",
   "metadata": {},
   "source": [
    "La sortie associée à la requête est un vecteur bidimensionnel puisque nous avons défini le nombre de colonnes de la matrice de poids correspondante, via `d_out`, à 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0b2963",
   "metadata": {},
   "source": [
    "Même si notre objectif immédiat est seulement de calculer le vecteur de contexte z(2), nous avons besoin des vecteurs clé et valeur pour tous les éléments de l'entrée car ils interviennent dans le calcul des poids d'attention par rapport à la requête q(2). Nous pouvons obtenir toutes les clés et toutes les valeurs par multiplication matricielle :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06646780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs@W_key\n",
    "values = inputs@W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60b604b",
   "metadata": {},
   "source": [
    "La deuxième étape consiste à calculer les scores d'attention.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "66764fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2@keys.T\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e033b37",
   "metadata": {},
   "source": [
    "Nous voulons maintenant passer des scores d'attention aux poids d'attention. Pour ce faire, nous normalisons les scores à l'aide de softmax après les avoir divisés par la racine carrée de la dimension d'embedding des clés.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6c79d8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1623, 0.1877, 0.1858, 0.1547, 0.1358, 0.1738])\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2/d_k**2, dim=-1)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d999a998",
   "metadata": {},
   "source": [
    "Nous pouvons désormais multiplier `attn_weights_2` par `values` pour obtenir le vecteur de contexte de sortie du token `x^2`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b9414197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2896, 0.7811])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2@values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7331e3f3",
   "metadata": {},
   "source": [
    "Nous venons de parcourir de nombreuses étapes pour calculer la sortie d'auto-attention. C'était principalement à des fins d'illustration pour détailler chaque étape. En pratique, dans la perspective de l'implémentation du LLM au chapitre suivant, il est utile d'organiser ce code dans une classe Python, comme montré ci-dessous.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bc38d492",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionV1(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_in:int, d_out:int):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        keys = x@self.W_key\n",
    "        queries = x@self.W_key\n",
    "        values = x@self.W_value\n",
    "        attn_scores = queries@keys.T\n",
    "        attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vecs = attn_weights@values\n",
    "        return context_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180b3ecc",
   "metadata": {},
   "source": [
    "Nous pouvons utiliser cette classe comme suit :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0c5e3a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2948, 0.7944],\n",
      "        [0.3013, 0.8099],\n",
      "        [0.3009, 0.8089],\n",
      "        [0.2927, 0.7888],\n",
      "        [0.2866, 0.7737],\n",
      "        [0.2979, 0.8016]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttentionV1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3523a512",
   "metadata": {},
   "source": [
    "Nous pouvons améliorer davantage l'implémentation `SelfAttentionV1` en utilisant les couches `nn.Linear` de PyTorch, qui réalisent effectivement la multiplication matricielle lorsque le biais est désactivé. Un avantage important d'`nn.Linear` par rapport à l'utilisation manuelle de `nn.Parameter(torch.rand(...))` est que `nn.Linear` bénéficie d'un schéma d'initialisation des poids optimisé, ce qui stabilise et rend l'entraînement plus efficace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "116ec7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionV2(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_in:int, d_out:int, qkv_bias:bool=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries@keys.T\n",
    "        attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vecs = attn_weights@values\n",
    "        return context_vecs\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e38e0a",
   "metadata": {},
   "source": [
    "`SelfAttentionV2` s'utilise de la même manière que `SelfAttentionV1` :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e7a3d0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0793,  0.0640],\n",
      "        [-0.0829,  0.0591],\n",
      "        [-0.0825,  0.0596],\n",
      "        [-0.0815,  0.0608],\n",
      "        [-0.0746,  0.0700],\n",
      "        [-0.0849,  0.0562]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttentionV2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6577f944",
   "metadata": {},
   "source": [
    "Pour vérifier que les deux implémentations, `SelfAttention_v1` et `SelfAttention_v2`, se comportent de façon similaire, nous pouvons transférer les matrices de poids d'un objet `SelfAttention_v2` vers un `SelfAttention_v1` afin que les deux produisent les mêmes résultats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f9aebc2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0793,  0.0640],\n",
       "        [-0.0829,  0.0591],\n",
       "        [-0.0825,  0.0596],\n",
       "        [-0.0815,  0.0608],\n",
       "        [-0.0746,  0.0700],\n",
       "        [-0.0849,  0.0562]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa_v1.W_query = torch.nn.Parameter(sa_v2.W_query.weight.T)\n",
    "sa_v1.W_key = torch.nn.Parameter(sa_v2.W_key.weight.T)\n",
    "sa_v1.W_value = torch.nn.Parameter(sa_v2.W_value.weight.T)\n",
    "sa_v1(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab18a076",
   "metadata": {},
   "source": [
    "### Masquer les mots futurs avec l'attention causale\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ac70e0",
   "metadata": {},
   "source": [
    "L'attention causale, également appelée attention masquée, est une forme spécialisée d'auto-attention. Elle contraint le modèle à ne considérer que les entrées passées et présentes d'une séquence lorsqu'il traite un token donné pour calculer les scores d'attention.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a87264",
   "metadata": {},
   "source": [
    "#### Appliquer un masque d'attention causale\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9b35a9",
   "metadata": {},
   "source": [
    "Une façon d'obtenir la matrice de poids d'attention masquée consiste à appliquer la fonction softmax aux scores d'attention, à annuler les éléments situés au-dessus de la diagonale puis à normaliser la matrice résultante.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4fcb8ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
       "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
       "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
       "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
       "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
       "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries@keys.T\n",
    "attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5, dim=-1)\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc9c8b8",
   "metadata": {},
   "source": [
    "Nous pouvons implémenter la deuxième étape avec la fonction `torch.tril`, qui crée un masque dont les valeurs au-dessus de la diagonale sont nulles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "abea74ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
       "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
       "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_lengt = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_lengt, context_lengt))\n",
    "masked_simple = attn_weights*mask_simple\n",
    "masked_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1a5d6659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
       "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
       "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_simple_norm = masked_simple/masked_simple.sum(dim=-1, keepdim=True)\n",
    "masked_simple_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1afc47",
   "metadata": {},
   "source": [
    "Lorsque nous appliquons un masque puis renormalisons les poids d'attention, on pourrait croire que l'information provenant des tokens futurs (ceux que nous voulons masquer) peut encore influencer le token courant car leurs valeurs interviennent dans le calcul du softmax. L'idée clé est que la renormalisation après masquage revient à recalculer le softmax sur un sous-ensemble plus petit, puisque les positions masquées ne contribuent plus à la valeur du softmax.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81431d97",
   "metadata": {},
   "source": [
    "Une méthode plus efficace pour obtenir la matrice de poids d'attention masquée consiste à appliquer des valeurs négatives infinies aux scores d'attention avant d'utiliser softmax.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "42cb25d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
       "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
       "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
       "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_lengt, context_lengt), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5078b94c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5366, 0.4634, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3660, 0.3168, 0.3172, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2681, 0.2473, 0.2474, 0.2371, 0.0000, 0.0000],\n",
       "        [0.2123, 0.1988, 0.1989, 0.1920, 0.1980, 0.0000],\n",
       "        [0.1853, 0.1665, 0.1667, 0.1578, 0.1667, 0.1569]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked/keys.shape[-1], dim=-1)\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c156bec5",
   "metadata": {},
   "source": [
    "#### Masquer des poids d'attention supplémentaires avec du dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966fa68d",
   "metadata": {},
   "source": [
    "Le dropout en apprentissage profond consiste à ignorer aléatoirement certaines unités cachées pendant l'entraînement, en les « supprimant » temporairement. Cette technique aide à éviter le surapprentissage en empêchant le modèle de dépendre excessivement d'un ensemble particulier d'unités cachées. Il est important de rappeler que le dropout n'est utilisé qu'à l'entraînement et qu'il est désactivé ensuite.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "43715c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2., 2., 2., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.],\n",
      "        [0., 0., 2., 0., 2., 0.],\n",
      "        [2., 2., 0., 0., 0., 2.],\n",
      "        [2., 0., 0., 0., 0., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5) #dropout 50% of tokens\n",
    "example = torch.ones(6, 6)\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf4c06c",
   "metadata": {},
   "source": [
    "Lorsqu'on applique un dropout de 50 % à une matrice de poids d'attention, la moitié des éléments est mise aléatoirement à zéro. Pour compenser cette réduction, les valeurs restantes sont multipliées par un facteur 1/0,5 = 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf49bd3c",
   "metadata": {},
   "source": [
    "Appliquons maintenant le dropout directement à la matrice de poids d'attention.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "44f8a755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5366, 0.4634, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3660, 0.3168, 0.3172, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2681, 0.2473, 0.2474, 0.2371, 0.0000, 0.0000],\n",
       "        [0.2123, 0.1988, 0.1989, 0.1920, 0.1980, 0.0000],\n",
       "        [0.1853, 0.1665, 0.1667, 0.1578, 0.1667, 0.1569]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "886bb021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9268, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6344, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5362, 0.4946, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4246, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3331, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df422a4d",
   "metadata": {},
   "source": [
    "### Implémenter une classe compacte d'attention causale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006871d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# CausalAttention : implémentation d'une attention \"causale\" de style GPT\n",
    "#\n",
    "# - Cette couche projette les entrées (x) en trois matrices : requêtes (Q),\n",
    "#   clés (K) et valeurs (V) via trois couches linéaires.\n",
    "#\n",
    "# - Le masque causal (matrice triangulaire supérieure) empêche chaque token\n",
    "#   d'accéder aux tokens futurs : un token t ne peut assister qu'aux positions\n",
    "#   ≤ t. Cela impose la propriété *auto-régressive*, essentielle pour les GPT.\n",
    "#\n",
    "# - Le score d’attention est calculé par Q @ Kᵀ puis normalisé via softmax\n",
    "#   (après l'application du masque).\n",
    "#\n",
    "# - On applique ensuite un dropout sur les poids d’attention pour régulariser.\n",
    "#\n",
    "# - Le vecteur de contexte est obtenu par : AttentionWeights @ V,\n",
    "#   ce qui permet à chaque token d'agréger l'information pertinente des tokens\n",
    "#   précédents.\n",
    "#\n",
    "# Résultat : une couche d’attention correcte pour un modèle de langage\n",
    "# auto-régressif, où chaque position ne dépend que du passé, jamais du futur.\n",
    "\n",
    "\n",
    "# register_buffer() permet de stocker un tenseur dans le module\n",
    "# sans le considérer comme un paramètre entraînable.\n",
    "# Avantages :\n",
    "# - déplacé automatiquement sur CPU/GPU avec model.to(device)\n",
    "# - sauvegardé dans state_dict()\n",
    "# - pas mis à jour par l’optimizer\n",
    "#\n",
    "# Idéal pour les masques causaux, constantes, etc.\n",
    "\n",
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e3c19b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_in:int, d_out:int, context_length:int, dropout:float, qkv_bias:bool=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        num_tokens = x.shape[1]\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries@keys.transpose(1, 2)\n",
    "        attn_scores.masked_fill_(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf\n",
    "        )\n",
    "        attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vecs = attn_weights@values\n",
    "        return context_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2d8fdc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9a6ad026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conttext_vecs: tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n",
      "\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"conttext_vecs:\", context_vecs)\n",
    "print(\"\\ncontext_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8986577f",
   "metadata": {},
   "source": [
    "## Étendre l'attention mono-tête à l'attention multi-tête\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fa6511",
   "metadata": {},
   "source": [
    "La dernière étape consiste à étendre la classe d'attention causale implémentée plus tôt à plusieurs têtes, d'où le terme d'attention multi-tête.\n",
    "\n",
    "Le terme « multi-tête » signifie que l'on découpe le mécanisme d'attention en plusieurs « têtes » qui opèrent indépendamment. Dans ce contexte, une seule instance d'attention causale correspond à une attention mono-tête, où un seul jeu de poids parcourt la séquence d'entrée.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6cd2cc",
   "metadata": {},
   "source": [
    "### Empiler plusieurs couches d'attention mono-tête\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26df12f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_in:int, d_out:int, context_length:int, dropout:int, num_heads:int, qkv_bias:bool=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [\n",
    "                CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) for _ in range(num_heads)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        \n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4abec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conttext_vecs: tensor([[[-0.4519,  0.2216,  0.4772,  0.1063,  0.4566,  0.2729],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257,  0.5792,  0.3011],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860,  0.6249,  0.3102],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589,  0.5691,  0.2785],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428,  0.5543,  0.2520],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493,  0.5337,  0.2499]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063,  0.4566,  0.2729],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257,  0.5792,  0.3011],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860,  0.6249,  0.3102],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589,  0.5691,  0.2785],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428,  0.5543,  0.2520],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493,  0.5337,  0.2499]]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "\n",
      "context_vecs.shape: torch.Size([2, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "mhaw = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=3)\n",
    "context_vecs = mhaw(batch)\n",
    "print(\"conttext_vecs:\", context_vecs)\n",
    "print(\"\\ncontext_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78266427",
   "metadata": {},
   "source": [
    "### Implémenter l'attention multi-tête avec découpe des poids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3ba357",
   "metadata": {},
   "source": [
    "Dans `MultiHeadAttentionWrapper`, plusieurs têtes sont obtenues en créant une liste d'objets `CausalAttention` (`self.heads`), chacun jouant le rôle d'une tête distincte. La classe `CausalAttention` exécute séparément le mécanisme d'attention, puis les résultats de chaque tête sont concaténés.\n",
    "\n",
    "À l'inverse, la classe `MultiHeadAttention` suivante intègre la logique multi-tête dans une seule classe : elle découpe les tenseurs requête, clé et valeur projetés en plusieurs têtes par reformatage, puis combine les résultats après le calcul de l'attention.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "44f0cdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_in:int, d_out:int, num_heads:int, context_length:int, dropout:float, qkv_bias:bool=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out//num_heads\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_keys = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "   \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        \n",
    "        num_batchs, num_tokens, d_in = x.shape\n",
    "        \n",
    "        keys = self.W_keys(x) # -> (num_batchs, num_tokens, d_out=num_heads*head_dim)\n",
    "        queries = self.W_query(x) # -> (num_batchs, num_tokens, d_out=num_heads*head_dim)\n",
    "        values = self.W_value(x) # -> (num_batchs, num_tokens, d_out=num_heads*head_dim)\n",
    "        print(f\"keys : {keys}\")\n",
    "        \n",
    "        keys = keys.view(num_batchs, num_tokens, self.num_heads, self.head_dim) # -> (num_batchs, num_tokens, num_heads, head_dim)\n",
    "        queries = queries.view(num_batchs, num_tokens, self.num_heads, self.head_dim)\n",
    "        values= values.view(num_batchs, num_tokens, self.num_heads, self.head_dim)\n",
    "        print(f\"keys : {keys}\")\n",
    "        \n",
    "        keys = keys.transpose(1, 2) # -> (num_batchs, num_heads, num_tokens, head_dim)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        print(f\"keys : {keys}\")\n",
    "        \n",
    "        attn_scores = queries@keys.transpose(-2, -1) # -> (num_batchs, num_heads, num_tokens, num_tokens)\n",
    "        attn_scores.masked_fill_(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf\n",
    "        )\n",
    "        print(f\"attn_scores : {attn_scores}\")\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        print(f\"Attn_weights : {attn_weights}\")\n",
    "        \n",
    "        print(f\"attn_weights@values : {attn_weights@values}\")\n",
    "        \n",
    "        context_vecs = (attn_weights@values).transpose(1, 2) # -> (num_batchs,  num_tokens, num_heads, head_dim)\n",
    "        \n",
    "        print(f\"context_vecs : {context_vecs}\")\n",
    "        \n",
    "        context_vecs = context_vecs.contiguous().view(num_batchs,  num_tokens, self.d_out) # -> (num_batchs,  num_tokens, d_out = num_heads*head_dim)\n",
    "        print(f\"context_vecs : {context_vecs}\")\n",
    "          \n",
    "        context_vecs = self.out_proj(context_vecs)\n",
    "        \n",
    "        return context_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "49f3a455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys : tensor([[[ 0.0618, -0.0698, -0.1751, -0.0632, -0.6651, -0.5935],\n",
      "         [-0.2447,  0.3075,  0.3275, -0.3468, -0.9567, -0.9076],\n",
      "         [-0.2248,  0.2904,  0.3332, -0.3266, -0.9397, -0.9034],\n",
      "         [-0.2204,  0.2532,  0.2174, -0.2655, -0.5439, -0.4879],\n",
      "         [ 0.1968, -0.0995,  0.3421,  0.1291, -0.3686, -0.5725],\n",
      "         [-0.4176,  0.4257,  0.1715, -0.4700, -0.7427, -0.5625]],\n",
      "\n",
      "        [[ 0.0618, -0.0698, -0.1751, -0.0632, -0.6651, -0.5935],\n",
      "         [-0.2447,  0.3075,  0.3275, -0.3468, -0.9567, -0.9076],\n",
      "         [-0.2248,  0.2904,  0.3332, -0.3266, -0.9397, -0.9034],\n",
      "         [-0.2204,  0.2532,  0.2174, -0.2655, -0.5439, -0.4879],\n",
      "         [ 0.1968, -0.0995,  0.3421,  0.1291, -0.3686, -0.5725],\n",
      "         [-0.4176,  0.4257,  0.1715, -0.4700, -0.7427, -0.5625]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "keys : tensor([[[[ 0.0618, -0.0698, -0.1751],\n",
      "          [-0.0632, -0.6651, -0.5935]],\n",
      "\n",
      "         [[-0.2447,  0.3075,  0.3275],\n",
      "          [-0.3468, -0.9567, -0.9076]],\n",
      "\n",
      "         [[-0.2248,  0.2904,  0.3332],\n",
      "          [-0.3266, -0.9397, -0.9034]],\n",
      "\n",
      "         [[-0.2204,  0.2532,  0.2174],\n",
      "          [-0.2655, -0.5439, -0.4879]],\n",
      "\n",
      "         [[ 0.1968, -0.0995,  0.3421],\n",
      "          [ 0.1291, -0.3686, -0.5725]],\n",
      "\n",
      "         [[-0.4176,  0.4257,  0.1715],\n",
      "          [-0.4700, -0.7427, -0.5625]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0618, -0.0698, -0.1751],\n",
      "          [-0.0632, -0.6651, -0.5935]],\n",
      "\n",
      "         [[-0.2447,  0.3075,  0.3275],\n",
      "          [-0.3468, -0.9567, -0.9076]],\n",
      "\n",
      "         [[-0.2248,  0.2904,  0.3332],\n",
      "          [-0.3266, -0.9397, -0.9034]],\n",
      "\n",
      "         [[-0.2204,  0.2532,  0.2174],\n",
      "          [-0.2655, -0.5439, -0.4879]],\n",
      "\n",
      "         [[ 0.1968, -0.0995,  0.3421],\n",
      "          [ 0.1291, -0.3686, -0.5725]],\n",
      "\n",
      "         [[-0.4176,  0.4257,  0.1715],\n",
      "          [-0.4700, -0.7427, -0.5625]]]], grad_fn=<ViewBackward0>)\n",
      "keys : tensor([[[[ 0.0618, -0.0698, -0.1751],\n",
      "          [-0.2447,  0.3075,  0.3275],\n",
      "          [-0.2248,  0.2904,  0.3332],\n",
      "          [-0.2204,  0.2532,  0.2174],\n",
      "          [ 0.1968, -0.0995,  0.3421],\n",
      "          [-0.4176,  0.4257,  0.1715]],\n",
      "\n",
      "         [[-0.0632, -0.6651, -0.5935],\n",
      "          [-0.3468, -0.9567, -0.9076],\n",
      "          [-0.3266, -0.9397, -0.9034],\n",
      "          [-0.2655, -0.5439, -0.4879],\n",
      "          [ 0.1291, -0.3686, -0.5725],\n",
      "          [-0.4700, -0.7427, -0.5625]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0618, -0.0698, -0.1751],\n",
      "          [-0.2447,  0.3075,  0.3275],\n",
      "          [-0.2248,  0.2904,  0.3332],\n",
      "          [-0.2204,  0.2532,  0.2174],\n",
      "          [ 0.1968, -0.0995,  0.3421],\n",
      "          [-0.4176,  0.4257,  0.1715]],\n",
      "\n",
      "         [[-0.0632, -0.6651, -0.5935],\n",
      "          [-0.3468, -0.9567, -0.9076],\n",
      "          [-0.3266, -0.9397, -0.9034],\n",
      "          [-0.2655, -0.5439, -0.4879],\n",
      "          [ 0.1291, -0.3686, -0.5725],\n",
      "          [-0.4700, -0.7427, -0.5625]]]], grad_fn=<TransposeBackward0>)\n",
      "attn_scores : tensor([[[[ 0.0821,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.0219, -0.0205,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.0240, -0.0255, -0.0331,    -inf,    -inf,    -inf],\n",
      "          [-0.0052,  0.0381,  0.0323,  0.0413,    -inf,    -inf],\n",
      "          [ 0.0543, -0.1069, -0.1112, -0.0636, -0.1573,    -inf],\n",
      "          [-0.0234,  0.0823,  0.0756,  0.0744, -0.0656,  0.1404]],\n",
      "\n",
      "         [[-0.2927,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.2968, -0.4532,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.3001, -0.4535, -0.4481,    -inf,    -inf,    -inf],\n",
      "          [-0.1307, -0.2117, -0.2071, -0.1253,    -inf,    -inf],\n",
      "          [-0.2736, -0.3312, -0.3339, -0.1569, -0.2883,    -inf],\n",
      "          [-0.1120, -0.2273, -0.2191, -0.1486, -0.0087, -0.2356]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0821,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.0219, -0.0205,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.0240, -0.0255, -0.0331,    -inf,    -inf,    -inf],\n",
      "          [-0.0052,  0.0381,  0.0323,  0.0413,    -inf,    -inf],\n",
      "          [ 0.0543, -0.1069, -0.1112, -0.0636, -0.1573,    -inf],\n",
      "          [-0.0234,  0.0823,  0.0756,  0.0744, -0.0656,  0.1404]],\n",
      "\n",
      "         [[-0.2927,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.2968, -0.4532,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.3001, -0.4535, -0.4481,    -inf,    -inf,    -inf],\n",
      "          [-0.1307, -0.2117, -0.2071, -0.1253,    -inf,    -inf],\n",
      "          [-0.2736, -0.3312, -0.3339, -0.1569, -0.2883,    -inf],\n",
      "          [-0.1120, -0.2273, -0.2191, -0.1486, -0.0087, -0.2356]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "Attn_weights : tensor([[[[1.1111, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5624, 0.5487, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3780, 0.3674, 0.3657, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2727, 0.0000, 0.2787, 0.2801, 0.0000, 0.0000],\n",
      "          [0.2395, 0.2182, 0.2177, 0.2237, 0.2120, 0.0000],\n",
      "          [0.1776, 0.1888, 0.1881, 0.1880, 0.1734, 0.1953]],\n",
      "\n",
      "         [[1.1111, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5806, 0.5305, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3922, 0.3589, 0.3600, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2839, 0.2709, 0.0000, 0.2848, 0.0000, 0.0000],\n",
      "          [0.2225, 0.0000, 0.2149, 0.2380, 0.2206, 0.0000],\n",
      "          [0.1900, 0.1778, 0.1786, 0.1861, 0.2017, 0.1769]]],\n",
      "\n",
      "\n",
      "        [[[1.1111, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.5487, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3780, 0.3674, 0.3657, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2727, 0.2796, 0.2787, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2395, 0.2182, 0.0000, 0.2237, 0.2120, 0.0000],\n",
      "          [0.1776, 0.0000, 0.1881, 0.1880, 0.0000, 0.1953]],\n",
      "\n",
      "         [[1.1111, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3922, 0.3589, 0.3600, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2839, 0.2709, 0.2716, 0.2848, 0.0000, 0.0000],\n",
      "          [0.2225, 0.0000, 0.2149, 0.2380, 0.2206, 0.0000],\n",
      "          [0.1900, 0.1778, 0.1786, 0.1861, 0.2017, 0.1769]]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "attn_weights@values : tensor([[[[-0.1289,  0.0667, -0.8332],\n",
      "          [-0.0021, -0.1889, -0.9166],\n",
      "          [ 0.0375, -0.2789, -0.9428],\n",
      "          [ 0.0303, -0.1649, -0.5836],\n",
      "          [ 0.0304, -0.3237, -0.7929],\n",
      "          [ 0.0684, -0.3019, -0.7614]],\n",
      "\n",
      "         [[ 0.2641, -0.1287, -0.1489],\n",
      "          [ 0.4285, -0.2048, -0.0969],\n",
      "          [ 0.4859, -0.2331, -0.0748],\n",
      "          [ 0.3052, -0.1412, -0.0563],\n",
      "          [ 0.3203, -0.1704, -0.0068],\n",
      "          [ 0.4309, -0.2105, -0.0320]]],\n",
      "\n",
      "\n",
      "        [[[-0.1289,  0.0667, -0.8332],\n",
      "          [ 0.0632, -0.2227, -0.4948],\n",
      "          [ 0.0375, -0.2789, -0.9428],\n",
      "          [ 0.0303, -0.2133, -0.7066],\n",
      "          [ 0.0071, -0.2329, -0.5976],\n",
      "          [ 0.0597, -0.1380, -0.4945]],\n",
      "\n",
      "         [[ 0.2641, -0.1287, -0.1489],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.4859, -0.2331, -0.0748],\n",
      "          [ 0.4532, -0.2126, -0.0633],\n",
      "          [ 0.3203, -0.1704, -0.0068],\n",
      "          [ 0.4309, -0.2105, -0.0320]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "context_vecs : tensor([[[[-0.1289,  0.0667, -0.8332],\n",
      "          [ 0.2641, -0.1287, -0.1489]],\n",
      "\n",
      "         [[-0.0021, -0.1889, -0.9166],\n",
      "          [ 0.4285, -0.2048, -0.0969]],\n",
      "\n",
      "         [[ 0.0375, -0.2789, -0.9428],\n",
      "          [ 0.4859, -0.2331, -0.0748]],\n",
      "\n",
      "         [[ 0.0303, -0.1649, -0.5836],\n",
      "          [ 0.3052, -0.1412, -0.0563]],\n",
      "\n",
      "         [[ 0.0304, -0.3237, -0.7929],\n",
      "          [ 0.3203, -0.1704, -0.0068]],\n",
      "\n",
      "         [[ 0.0684, -0.3019, -0.7614],\n",
      "          [ 0.4309, -0.2105, -0.0320]]],\n",
      "\n",
      "\n",
      "        [[[-0.1289,  0.0667, -0.8332],\n",
      "          [ 0.2641, -0.1287, -0.1489]],\n",
      "\n",
      "         [[ 0.0632, -0.2227, -0.4948],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0375, -0.2789, -0.9428],\n",
      "          [ 0.4859, -0.2331, -0.0748]],\n",
      "\n",
      "         [[ 0.0303, -0.2133, -0.7066],\n",
      "          [ 0.4532, -0.2126, -0.0633]],\n",
      "\n",
      "         [[ 0.0071, -0.2329, -0.5976],\n",
      "          [ 0.3203, -0.1704, -0.0068]],\n",
      "\n",
      "         [[ 0.0597, -0.1380, -0.4945],\n",
      "          [ 0.4309, -0.2105, -0.0320]]]], grad_fn=<TransposeBackward0>)\n",
      "context_vecs : tensor([[[-0.1289,  0.0667, -0.8332,  0.2641, -0.1287, -0.1489],\n",
      "         [-0.0021, -0.1889, -0.9166,  0.4285, -0.2048, -0.0969],\n",
      "         [ 0.0375, -0.2789, -0.9428,  0.4859, -0.2331, -0.0748],\n",
      "         [ 0.0303, -0.1649, -0.5836,  0.3052, -0.1412, -0.0563],\n",
      "         [ 0.0304, -0.3237, -0.7929,  0.3203, -0.1704, -0.0068],\n",
      "         [ 0.0684, -0.3019, -0.7614,  0.4309, -0.2105, -0.0320]],\n",
      "\n",
      "        [[-0.1289,  0.0667, -0.8332,  0.2641, -0.1287, -0.1489],\n",
      "         [ 0.0632, -0.2227, -0.4948,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0375, -0.2789, -0.9428,  0.4859, -0.2331, -0.0748],\n",
      "         [ 0.0303, -0.2133, -0.7066,  0.4532, -0.2126, -0.0633],\n",
      "         [ 0.0071, -0.2329, -0.5976,  0.3203, -0.1704, -0.0068],\n",
      "         [ 0.0597, -0.1380, -0.4945,  0.4309, -0.2105, -0.0320]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4615,  0.0466, -0.0957, -0.0997,  0.4638, -0.1654],\n",
       "         [ 0.5027,  0.0054, -0.2106, -0.0236,  0.5276, -0.1192],\n",
       "         [ 0.5174, -0.0086, -0.2539,  0.0018,  0.5512, -0.1015],\n",
       "         [ 0.4743, -0.0484, -0.1432, -0.0860,  0.4775, -0.0169],\n",
       "         [ 0.4799, -0.0101, -0.2614, -0.0658,  0.5580, -0.0273],\n",
       "         [ 0.5062, -0.0405, -0.2377, -0.0285,  0.5370, -0.0331]],\n",
       "\n",
       "        [[ 0.4615,  0.0466, -0.0957, -0.0997,  0.4638, -0.1654],\n",
       "         [ 0.4000, -0.0204, -0.1297, -0.1882,  0.4936,  0.0512],\n",
       "         [ 0.5174, -0.0086, -0.2539,  0.0018,  0.5512, -0.1015],\n",
       "         [ 0.5103, -0.0508, -0.1956, -0.0288,  0.5020, -0.0481],\n",
       "         [ 0.4813, -0.0433, -0.2037, -0.0852,  0.5100,  0.0032],\n",
       "         [ 0.5091, -0.0790, -0.1353, -0.0593,  0.4673,  0.0006]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha = MultiHeadAttention(d_in=3, d_out=6, num_heads=2, context_length=6, dropout=0.1)\n",
    "mha(batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
