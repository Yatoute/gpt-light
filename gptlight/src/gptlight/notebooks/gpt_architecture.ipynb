{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44d08e02",
   "metadata": {},
   "source": [
    "## Implémenter un modèle GPT from scratch pour générer du texte\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2a7060",
   "metadata": {},
   "source": [
    "Nous avons déjà étudié et codé le mécanisme d'attention multi-tête, l'un des composants centraux des LLM. Nous allons maintenant coder les autres briques d'un LLM et les assembler dans un modèle de type GPT que nous entraînerons au prochain chapitre pour générer du texte proche de celui d'un humain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb02292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from gptlight.tokenizer import GPTTokenizer\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ac0443",
   "metadata": {},
   "source": [
    "### Coder une architecture de LLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14110f3",
   "metadata": {},
   "source": [
    "Nous définissons la configuration du petit modèle GPT-2 via le dictionnaire Python suivant, que nous réutiliserons plus loin dans les exemples de code :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec111029",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257, # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768, # Embedding dimension\n",
    "    \"n_heads\": 12, # Number of attention heads\n",
    "    \"n_layers\": 12, # Number of layers\n",
    "    \"drop_rate\": 0.1, # Dropout rate\n",
    "    \"qkv_bias\": False # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79b502d",
   "metadata": {},
   "source": [
    "- `vocab_size` correspond à un vocabulaire de 50 257 mots, comme celui utilisé par le tokenizer BPE.\n",
    "- `context_length` désigne le nombre maximal de tokens que le modèle peut traiter grâce aux embeddings positionnels.\n",
    "- `emb_dim` représente la taille des embeddings, en transformant chaque token en un vecteur de dimension 768.\n",
    "- `n_heads` indique le nombre de têtes d'attention dans le mécanisme multi-tête.\n",
    "- `n_layers` précise le nombre de blocs transformeur dans le modèle, que nous détaillerons ensuite.\n",
    "- `drop_rate` exprime l'intensité du dropout (0,1 signifie qu'environ 10 % des unités cachées sont mises à zéro de façon aléatoire) afin de limiter le surapprentissage.\n",
    "- `qkv_bias` détermine s'il faut inclure un biais dans les couches linéaires du multi-head attention pour les calculs de requêtes, clés et valeurs. Nous partons par défaut sans biais, conformément aux usages des LLM modernes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d234e5",
   "metadata": {},
   "source": [
    "Nous commençons par l'ossature GPT, une architecture de départ, avant d'étudier chaque brique centrale et de les assembler en bloc transformeur pour obtenir l'architecture finale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61440fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyGPTModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_block = nn.Sequential(\n",
    "            *[\n",
    "                DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])\n",
    "            ]\n",
    "        )\n",
    "        self.final_norm = DummyLayernorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "    \n",
    "    def forward(self, in_idx):\n",
    "        \n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(\n",
    "            torch.arange(seq_len, device=in_idx.device)\n",
    "        )\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_block(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        \n",
    "        return logits\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, cfg:dict):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class DummyLayernorm(nn.Module):\n",
    "    \n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2d67ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6109, 3626, 6100,  345],\n",
       "        [6109, 1110, 6622,  257]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPTTokenizer()\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "\n",
    "batch = torch.stack(batch, dim=0)\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14e60757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape : torch.Size([2, 4, 50257])\n",
      "tensor([[[-0.9289,  0.2748, -0.7557,  ..., -1.6070,  0.2702, -0.5888],\n",
      "         [-0.4476,  0.1726,  0.5354,  ..., -0.3932,  1.5285,  0.8557],\n",
      "         [ 0.5680,  1.6053, -0.2155,  ...,  1.1624,  0.1380,  0.7425],\n",
      "         [ 0.0447,  2.4787, -0.8843,  ...,  1.3219, -0.0864, -0.5856]],\n",
      "\n",
      "        [[-1.5474, -0.0542, -1.0571,  ..., -1.8061, -0.4494, -0.6747],\n",
      "         [-0.8422,  0.8243, -0.1098,  ..., -0.1434,  0.2079,  1.2046],\n",
      "         [ 0.1355,  1.1858, -0.1453,  ...,  0.0869, -0.1590,  0.1552],\n",
      "         [ 0.1666, -0.8138,  0.2307,  ...,  2.5035, -0.3055, -0.3083]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(f\"Output shape : {logits.shape}\")\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19af205a",
   "metadata": {},
   "source": [
    "Le tenseur de sortie comporte deux lignes correspondant aux deux exemples de texte. Chaque exemple contient quatre tokens ; chaque token est encodé comme un vecteur de dimension 50 257, ce qui correspond à la taille du vocabulaire du tokenizer.\n",
    "\n",
    "L'embedding possède 50 257 dimensions car chacune représente un token unique du vocabulaire. Lorsque nous implémenterons le post-traitement, nous convertirons ces vecteurs à 50 257 dimensions en identifiants de tokens, que nous pourrons ensuite décoder en mots.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c16f41",
   "metadata": {},
   "source": [
    "### Normaliser les activations avec une layer normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fed977",
   "metadata": {},
   "source": [
    "L'entraînement de réseaux profonds comportant de nombreuses couches peut se révéler difficile à cause de phénomènes tels que l'explosion ou la disparition du gradient. Ces problèmes rendent l'entraînement instable et compliquent l'ajustement efficace des poids, ce qui empêche le réseau de trouver un jeu de paramètres minimisant la fonction de perte.\n",
    "\n",
    "Implémentons maintenant la layer normalization pour améliorer la stabilité et l'efficacité de l'entraînement. L'idée principale consiste à ajuster les activations (les sorties) d'une couche de réseau afin qu'elles aient une moyenne nulle et une variance unitaire.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8d1208",
   "metadata": {},
   "source": [
    "> Exemple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20d4dba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out: tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "\n",
      "out_norm :tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2, 5)\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(f\"out: {out}\")\n",
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "out_norm = (out-mean)/torch.sqrt(var)\n",
    "print(f\"\\nout_norm :{out_norm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b263ad",
   "metadata": {},
   "source": [
    "> Une classe de layer normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "43f254b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb_dim:int):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "    \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        \n",
    "        x_mean = x.mean(dim=-1, keepdim=True)\n",
    "        x_var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        x_norm = (x-x_mean)/torch.sqrt(x_var + self.eps)\n",
    "        \n",
    "        return self.scale*x_norm + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a338b39",
   "metadata": {},
   "source": [
    "Les paramètres de mise à l'échelle (scale) et de décalage (shift) sont entraînables (de la même dimension que l'entrée) et le LLM les ajuste automatiquement pendant l'entraînement si cela améliore ses performances.\n",
    "\n",
    "Ainsi, le modèle peut apprendre la mise à l'échelle et le décalage les plus adaptés aux données qu'il traite.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9cbfa4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_norm : tensor([[ 0.5528,  1.0693, -0.0223,  0.2656, -1.8654],\n",
      "        [ 0.9087, -1.3767, -0.9564,  1.1304,  0.2940]], grad_fn=<AddBackward0>)\n",
      "\n",
      "mean : tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "\n",
      "var : tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "layer_norm = LayerNorm(emb_dim=5)\n",
    "out_norm = layer_norm(batch_example)\n",
    "print(f\"out_norm : {out_norm}\")\n",
    "\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "print(f\"\\nmean : {mean}\")\n",
    "\n",
    "var = out_norm.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(f\"\\nvar : {var}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39076780",
   "metadata": {},
   "source": [
    "### Implémenter un réseau feed-forward avec des activations GELU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa5d3a5",
   "metadata": {},
   "source": [
    "Historiquement, la fonction d'activation ReLU a été largement utilisée en apprentissage profond pour sa simplicité et son efficacité. Cependant, dans les LLM, d'autres activations sont également employées, notamment GELU (Gaussian Error Linear Unit) et SwiGLU (Swish-Gated Linear Unit).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db5c817",
   "metadata": {},
   "source": [
    "La fonction GELU peut être implémentée de plusieurs façons ; la version exacte se définit par GELU(x) = x·Φ(x), où Φ(x) est la fonction de répartition d'une loi normale standard. En pratique, on utilise souvent une approximation moins coûteuse à calculer :\n",
    "\n",
    "$$\n",
    "\\mathrm{GELU}(x) \\approx 0.5\\,x \\left( 1 + \\tanh\\left[ \\sqrt{\\frac{2}{\\pi}} \\left( x + 0.044715\\,x^3 \\right) \\right] \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99507998",
   "metadata": {},
   "source": [
    "> Implémentation de la fonction d'activation GELU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bdaf7071",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        \n",
    "        return 0.5*x*(\n",
    "            1 + torch.tanh(\n",
    "                torch.sqrt(torch.tensor(2/torch.pi))*(\n",
    "                    x + 0.044715*torch.pow(x, 3)\n",
    "                )\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e455e1",
   "metadata": {},
   "source": [
    "Utilisons maintenant GELU pour implémenter le petit module FeedForward que nous intégrerons plus tard dans le bloc transformeur du LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766fbe2b",
   "metadata": {},
   "source": [
    "> Un module de réseau feed-forward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d537e6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4*cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4*cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3c861f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768)\n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f32560",
   "metadata": {},
   "source": [
    "### Ajouter des connexions de raccourci\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab3ed4c",
   "metadata": {},
   "source": [
    "À l'origine, les connexions de raccourci ont été introduites dans les réseaux profonds de vision (notamment les réseaux résiduels) pour atténuer le problème de disparition du gradient. Ce problème apparaît lorsque les gradients, qui guident les mises à jour de poids, deviennent de plus en plus faibles en remontant les couches, ce qui rend difficile l'entraînement des premières couches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61629973",
   "metadata": {},
   "source": [
    "Les connexions de raccourci consistent à ajouter l'entrée d'une couche à sa sortie, créant ainsi un chemin alternatif qui contourne certaines couches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb2c1e7",
   "metadata": {},
   "source": [
    "> Réseau illustrant les connexions de raccourci\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "36033efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, layer_sizes, use_shortcut:bool):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "                nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "                nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "                nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "                nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())   \n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer(x)\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64492487",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cdf83484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gradients(model, x):\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "    \n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    \n",
    "    loss.backward()\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6dec1df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173584925942123\n",
      "layers.1.0.weight has gradient mean of 0.00012011159560643137\n",
      "layers.2.0.weight has gradient mean of 0.0007152040489017963\n",
      "layers.3.0.weight has gradient mean of 0.0013988736318424344\n",
      "layers.4.0.weight has gradient mean of 0.005049645435065031\n"
     ]
    }
   ],
   "source": [
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7e7da599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22169791162014008\n",
      "layers.1.0.weight has gradient mean of 0.20694105327129364\n",
      "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
      "layers.3.0.weight has gradient mean of 0.2665732204914093\n",
      "layers.4.0.weight has gradient mean of 1.3258540630340576\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "layer_sizes, use_shortcut=True\n",
    ")\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2509e7c",
   "metadata": {},
   "source": [
    "En résumé, les connexions de raccourci sont essentielles pour dépasser les limitations liées à la disparition du gradient dans les réseaux profonds. Elles constituent une brique incontournable des grands modèles comme les LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c087a3e",
   "metadata": {},
   "source": [
    "### Relier attention et couches linéaires dans un bloc transformeur\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7f3a76",
   "metadata": {},
   "source": [
    "Ce bloc, répété une douzaine de fois dans l'architecture GPT-2 à 124 millions de paramètres, combine plusieurs notions vues précédemment : attention multi-tête, layer normalization, dropout, couches feed-forward et activations GELU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be24742b",
   "metadata": {},
   "source": [
    "> Le bloc transformeur composant de GPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef5826c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gptlight.models.transformer.normalization import LayerNorm\n",
    "from gptlight.models.transformer.attention import MultiHeadAttention\n",
    "from gptlight.models.transformer.ffn import FeedForward\n",
    "\n",
    "from gptlight.config import GPTConfig, GPT2_CONFIG_124M\n",
    "\n",
    "class GPTTransformerBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, cfg:GPTConfig):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg.emb_dim,\n",
    "            d_out=cfg.emb_dim,\n",
    "            num_heads=cfg.n_heads,\n",
    "            context_length=cfg.context_length,\n",
    "            dropout=cfg.drop_rate,\n",
    "            qkv_bias=cfg.qkv_bias\n",
    "        )\n",
    "        \n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(emb_dim=cfg.emb_dim)\n",
    "        self.norm2 = LayerNorm(emb_dim=cfg.emb_dim)\n",
    "        self.drop_shortcut = nn.Dropout(cfg.drop_rate)\n",
    "    \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        \n",
    "        shortcut =  x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x) \n",
    "        x = x + shortcut\n",
    "        \n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        \n",
    "        return x     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1d65efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([2, 4, 768])\n",
      "Output shape:  torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)\n",
    "block = GPTTransformerBlock(GPT2_CONFIG_124M)\n",
    "output = block(x)\n",
    "print(\"Input shape: \", x.shape)\n",
    "print(\"Output shape: \", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db19b162",
   "metadata": {},
   "source": [
    "### Coder le modèle GPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f10606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gptlight.config import GPTConfig, GPT2_CONFIG_124M\n",
    "from gptlight.models.transformer.normalization import LayerNorm\n",
    "from gptlight.models.transformer import GPTTransformerBlock\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, cfg:GPTConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.tok_emb = nn.Embedding(cfg.vocab_size, cfg.emb_dim)\n",
    "        self.pos_emb = nn.Embedding(cfg.context_length, cfg.emb_dim)\n",
    "        self.drop_emb = nn.Dropout(cfg.drop_rate)\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[\n",
    "                GPTTransformerBlock(cfg) for _ in range(cfg.n_layers)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg.emb_dim)\n",
    "        self.out_head = nn.Linear(cfg.emb_dim, cfg.vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, in_idx):\n",
    "        \n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(\n",
    "            torch.arange(seq_len, device=in_idx.device)\n",
    "        )\n",
    "        x = tok_embeds + pos_embeds\n",
    "        \n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        \n",
    "        return logits\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dce25dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.1381,  0.0077, -0.1963,  ..., -0.0222, -0.1060,  0.1717],\n",
      "         [ 0.3865, -0.8408, -0.6564,  ..., -0.5163,  0.2369, -0.3357],\n",
      "         [ 0.6989, -0.1829, -0.1631,  ...,  0.1472, -0.6504, -0.0056],\n",
      "         [-0.4290,  0.1669, -0.1258,  ...,  1.1579,  0.5303, -0.5549]],\n",
      "\n",
      "        [[ 0.1094, -0.2894, -0.1467,  ..., -0.0557,  0.2911, -0.2824],\n",
      "         [ 0.0882, -0.3552, -0.3527,  ...,  1.2930,  0.0053,  0.1898],\n",
      "         [ 0.6091,  0.4702, -0.4094,  ...,  0.7688,  0.3787, -0.1974],\n",
      "         [-0.0612, -0.0737,  0.4751,  ...,  1.2463, -0.3834,  0.0609]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT2_CONFIG_124M)\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2a338a",
   "metadata": {},
   "source": [
    "À l'aide de la méthode `numel()` (abréviation de « number of elements »), nous pouvons compter le nombre total de paramètres contenus dans les tenseurs du modèle :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4990bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75023893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14bf7ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 124,412,160\n"
     ]
    }
   ],
   "source": [
    "total_params_gpt2 = (\n",
    "total_params - sum(p.numel()\n",
    "for p in model.out_head.parameters())\n",
    ")\n",
    "print(f\"Number of trainable parameters \"\n",
    "f\"considering weight tying: {total_params_gpt2:,}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa154abb",
   "metadata": {},
   "source": [
    "Pour finir, calculons l'empreinte mémoire des 163 millions de paramètres de notre objet `GPTModel` :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7595d007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "# Calculates the total size in bytes (assuming float32, 4 bytes per parameter)\n",
    "total_size_bytes = total_params*4\n",
    "total_size_mb = total_size_bytes/(1024*1024)\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b181f9",
   "metadata": {},
   "source": [
    "Ainsi, en supposant que chaque paramètre est un flottant 32 bits occupant 4 octets, la taille totale du modèle atteint 621,83 Mo, ce qui illustre l'espace de stockage déjà conséquent requis par un LLM pourtant relativement petit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c93ac5b",
   "metadata": {},
   "source": [
    "### Générer du texte\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac6b1cd",
   "metadata": {},
   "source": [
    "> Fonction permettant au modèle GPT de générer du texte\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd2e1e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "        \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efbbbdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ede947",
   "metadata": {},
   "source": [
    "Nous plaçons ensuite le modèle en mode `.eval()`. Cela désactive les composantes aléatoires comme le dropout, réservé à l'entraînement, puis nous appliquons `generate_text_simple` sur le tenseur d'entrée encodé :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d382fbc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "model.eval() #Disables dropout since we are not training the model\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor,\n",
    "    max_new_tokens=6,\n",
    "    context_size=GPT2_CONFIG_124M.context_length\n",
    ")\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7c2857",
   "metadata": {},
   "source": [
    "Avec la méthode `.decode` du tokenizer, nous pouvons reconvertir les identifiants en texte :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0ad7072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Featureiman Byeswickattribute argue\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795e32e1",
   "metadata": {},
   "source": [
    "On constate que le modèle génère du charabia, bien loin d'un texte cohérent du type « Hello, I am a model ready to help. What happened? ». La raison est simple : le modèle n'a pas encore été entraîné.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
