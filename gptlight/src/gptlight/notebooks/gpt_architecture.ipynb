{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44d08e02",
   "metadata": {},
   "source": [
    "## Implementing a GPT model from scratch to generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2a7060",
   "metadata": {},
   "source": [
    "You’ve already learned and coded the multi-head attention mechanism, one of the\n",
    "core components of LLMs. Now, we will code the other building blocks of an LLM\n",
    "and assemble them into a GPT-like model that we will train in the next chapter to\n",
    "generate human-like text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb02292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from gptlight.tokenizer import GPTTokenizer\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ac0443",
   "metadata": {},
   "source": [
    "### Coding an LLM architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14110f3",
   "metadata": {},
   "source": [
    "We specify the configuration of the small GPT-2 model via the following Python dictionary, which we will use in the code examples later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec111029",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257, # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768, # Embedding dimension\n",
    "    \"n_heads\": 12, # Number of attention heads\n",
    "    \"n_layers\": 12, # Number of layers\n",
    "    \"drop_rate\": 0.1, # Dropout rate\n",
    "    \"qkv_bias\": False # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79b502d",
   "metadata": {},
   "source": [
    "- `vocab_size` refers to a vocabulary of 50,257 words, as used by the BPE tokenizer\n",
    "- `context_length` denotes the maximum number of input tokens the model can handle via the positional embeddings.\n",
    "- `emb_dim` represents the embedding size, transforming each token into a 768-dimensional vector.\n",
    "- `n_heads` indicates the count of attention heads in the multi-head attention mechanism.\n",
    "- `n_layers` specifies the number of transformer blocks in the model, which we will cover in the upcoming discussion.\n",
    "- `drop_rate` indicates the intensity of the dropout mechanism (0.1 implies a 10% random drop out of hidden units) to prevent overfitting.\n",
    "- `qkv_bias` determines whether to include a bias vector in the Linear layers of the multi-head attention for query, key, and value computations. We will initially disable this, following the norms of modern LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d234e5",
   "metadata": {},
   "source": [
    "We start with the GPT backbone, a placeholder architecture, before getting to the individual core pieces and eventually assembling them in a transformer block for the final GPT architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61440fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyGPTModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_block = nn.Sequential(\n",
    "            *[\n",
    "                DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])\n",
    "            ]\n",
    "        )\n",
    "        self.final_norm = DummyLayernorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "    \n",
    "    def forward(self, in_idx):\n",
    "        \n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(\n",
    "            torch.arange(seq_len, device=in_idx.device)\n",
    "        )\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_block(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        \n",
    "        return logits\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, cfg:dict):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class DummyLayernorm(nn.Module):\n",
    "    \n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2d67ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6109, 3626, 6100,  345],\n",
       "        [6109, 1110, 6622,  257]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPTTokenizer()\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "\n",
    "batch = torch.stack(batch, dim=0)\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14e60757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape : torch.Size([2, 4, 50257])\n",
      "tensor([[[-0.9289,  0.2748, -0.7557,  ..., -1.6070,  0.2702, -0.5888],\n",
      "         [-0.4476,  0.1726,  0.5354,  ..., -0.3932,  1.5285,  0.8557],\n",
      "         [ 0.5680,  1.6053, -0.2155,  ...,  1.1624,  0.1380,  0.7425],\n",
      "         [ 0.0447,  2.4787, -0.8843,  ...,  1.3219, -0.0864, -0.5856]],\n",
      "\n",
      "        [[-1.5474, -0.0542, -1.0571,  ..., -1.8061, -0.4494, -0.6747],\n",
      "         [-0.8422,  0.8243, -0.1098,  ..., -0.1434,  0.2079,  1.2046],\n",
      "         [ 0.1355,  1.1858, -0.1453,  ...,  0.0869, -0.1590,  0.1552],\n",
      "         [ 0.1666, -0.8138,  0.2307,  ...,  2.5035, -0.3055, -0.3083]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(f\"Output shape : {logits.shape}\")\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19af205a",
   "metadata": {},
   "source": [
    "The output tensor has two rows corresponding to the two text samples. Each text sample consists of four tokens; each token is a 50,257-dimensional vector, which matches the size of the tokenizer’s vocabulary.\n",
    "\n",
    "The embedding has 50,257 dimensions because each of these dimensions refers to\n",
    "a unique token in the vocabulary. When we implement the postprocessing code, we\n",
    "will convert these 50,257-dimensional vectors back into token IDs, which we can then decode into words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c16f41",
   "metadata": {},
   "source": [
    "### Normalizing activations with layer normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fed977",
   "metadata": {},
   "source": [
    "Training deep neural networks with many layers can sometimes prove challenging\n",
    "due to problems like vanishing or exploding gradients. These problems lead to unstable training dynamics and make it difficult for the network to effectively adjust its weights, which means the learning process struggles to find a set of parameters(weights) for the neural network that minimizes the loss function. \n",
    "\n",
    "Let’s now implement layer normalization to improve the stability and efficiency of neural network training. The main idea behind layer normalization is to adjust the activations (outputs) of a neural network layer to have a mean of 0 and a variance of 1, also known as unit variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8d1208",
   "metadata": {},
   "source": [
    "> Exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20d4dba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out: tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "\n",
      "out_norm :tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2, 5)\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(f\"out: {out}\")\n",
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "out_norm = (out-mean)/torch.sqrt(var)\n",
    "print(f\"\\nout_norm :{out_norm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b263ad",
   "metadata": {},
   "source": [
    "> A layer normalization class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "43f254b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb_dim:int):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "    \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        \n",
    "        x_mean = x.mean(dim=-1, keepdim=True)\n",
    "        x_var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        x_norm = (x-x_mean)/torch.sqrt(x_var + self.eps)\n",
    "        \n",
    "        return self.scale*x_norm + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a338b39",
   "metadata": {},
   "source": [
    "The scale and shift are two trainable parameters (of the same dimension as the input) that the LLM automatically adjusts during training if it is determined that doing so would improve the model’s performance on its training task.\n",
    "\n",
    "This allows the model to learn appropriate scaling and shifting that best suit the data it is processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9cbfa4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_norm : tensor([[ 0.5528,  1.0693, -0.0223,  0.2656, -1.8654],\n",
      "        [ 0.9087, -1.3767, -0.9564,  1.1304,  0.2940]], grad_fn=<AddBackward0>)\n",
      "\n",
      "mean : tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "\n",
      "var : tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "layer_norm = LayerNorm(emb_dim=5)\n",
    "out_norm = layer_norm(batch_example)\n",
    "print(f\"out_norm : {out_norm}\")\n",
    "\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "print(f\"\\nmean : {mean}\")\n",
    "\n",
    "var = out_norm.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(f\"\\nvar : {var}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39076780",
   "metadata": {},
   "source": [
    "### Implementing a feed forward network with GELU activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa5d3a5",
   "metadata": {},
   "source": [
    "Historically, the ReLU activation function has been commonly used in deep learning due to its simplicity and effectiveness across various neural network architectures.\n",
    "However, in LLMs, several other activation functions are employed beyond the traditional ReLU. Two notable examples are GELU (Gaussian error linear unit) and SwiGLU (Swish-gated linear unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db5c817",
   "metadata": {},
   "source": [
    "The GELU activation function can be implemented in several ways; the exact ver-\n",
    "sion is defined as GELU(x) = x⋅Φ(x), where Φ(x) is the cumulative distribution function of the standard Gaussian distribution. In practice, however, it’s common to implement a computationally cheaper approximation :\n",
    "\n",
    "$$\n",
    "\\mathrm{GELU}(x) \\approx 0.5\\,x \\left( 1 + \\tanh\\left[ \\sqrt{\\frac{2}{\\pi}} \\left( x + 0.044715\\,x^3 \\right) \\right] \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99507998",
   "metadata": {},
   "source": [
    "> An implementation of the GELU activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bdaf7071",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        \n",
    "        return 0.5*x*(\n",
    "            1 + torch.tanh(\n",
    "                torch.sqrt(torch.tensor(2/torch.pi))*(\n",
    "                    x + 0.044715*torch.pow(x, 3)\n",
    "                )\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e455e1",
   "metadata": {},
   "source": [
    "Next, let’s use the GELU function to implement the small neural network module, FeedForward, that we will be using in the LLM’s transformer block later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766fbe2b",
   "metadata": {},
   "source": [
    "> A feed forward neural network module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d537e6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4*cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4*cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3c861f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768)\n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f32560",
   "metadata": {},
   "source": [
    "### Adding shortcut connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab3ed4c",
   "metadata": {},
   "source": [
    "Originally, shortcut connections were proposed for deep networks in computer vision (specifically, in residual networks) to mitigate the challenge of vanishing gradients. The vanishing gradient problem refers to the issue where gradients (which guide weight updates during training) become progressively smaller as they propagate backward through the layers, making it difficult to effectively train earlier layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61629973",
   "metadata": {},
   "source": [
    "Shortcut connections involve adding the inputs of a layer to its outputs, effectively creating an alternate path that bypasses certain layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb2c1e7",
   "metadata": {},
   "source": [
    "> A neural network to illustrate shortcut connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "36033efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, layer_sizes, use_shortcut:bool):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "                nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "                nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "                nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "                nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())   \n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer(x)\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64492487",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cdf83484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gradients(model, x):\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "    \n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    \n",
    "    loss.backward()\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6dec1df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173584925942123\n",
      "layers.1.0.weight has gradient mean of 0.00012011159560643137\n",
      "layers.2.0.weight has gradient mean of 0.0007152040489017963\n",
      "layers.3.0.weight has gradient mean of 0.0013988736318424344\n",
      "layers.4.0.weight has gradient mean of 0.005049645435065031\n"
     ]
    }
   ],
   "source": [
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7e7da599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22169791162014008\n",
      "layers.1.0.weight has gradient mean of 0.20694105327129364\n",
      "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
      "layers.3.0.weight has gradient mean of 0.2665732204914093\n",
      "layers.4.0.weight has gradient mean of 1.3258540630340576\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "layer_sizes, use_shortcut=True\n",
    ")\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2509e7c",
   "metadata": {},
   "source": [
    "In conclusion, shortcut connections are important for overcoming the limitations posed by the vanishing gradient problem in deep neural networks. Shortcut connections are a core building block of very large models such as LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c087a3e",
   "metadata": {},
   "source": [
    "### Connecting attention and linear layers in a transformer block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7f3a76",
   "metadata": {},
   "source": [
    "This block, which is repeated a dozen times in the 124-million-\n",
    "parameter GPT-2 architecture, combines several concepts we have previously covered:\n",
    "multi-head attention, layer normalization, dropout, feed forward layers, and GELU\n",
    "activations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be24742b",
   "metadata": {},
   "source": [
    "> The transformer block component of GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef5826c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gptlight.models.transformer.normalization import LayerNorm\n",
    "from gptlight.models.transformer.attention import MultiHeadAttention\n",
    "from gptlight.models.transformer.ffn import FeedForward\n",
    "\n",
    "from gptlight.config import GPTConfig, GPT2_CONFIG_124M\n",
    "\n",
    "class GPTTransformerBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, cfg:GPTConfig):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg.emb_dim,\n",
    "            d_out=cfg.emb_dim,\n",
    "            num_heads=cfg.n_heads,\n",
    "            context_length=cfg.context_length,\n",
    "            dropout=cfg.drop_rate,\n",
    "            qkv_bias=cfg.qkv_bias\n",
    "        )\n",
    "        \n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(emb_dim=cfg.emb_dim)\n",
    "        self.norm2 = LayerNorm(emb_dim=cfg.emb_dim)\n",
    "        self.drop_shortcut = nn.Dropout(cfg.drop_rate)\n",
    "    \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        \n",
    "        shortcut =  x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x) \n",
    "        x = x + shortcut\n",
    "        \n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        \n",
    "        return x     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1d65efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([2, 4, 768])\n",
      "Output shape:  torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)\n",
    "block = GPTTransformerBlock(GPT2_CONFIG_124M)\n",
    "output = block(x)\n",
    "print(\"Input shape: \", x.shape)\n",
    "print(\"Output shape: \", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db19b162",
   "metadata": {},
   "source": [
    "### Coding the GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f10606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gptlight.config import GPTConfig, GPT2_CONFIG_124M\n",
    "from gptlight.models.transformer.normalization import LayerNorm\n",
    "from gptlight.models.transformer import GPTTransformerBlock\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, cfg:GPTConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.tok_emb = nn.Embedding(cfg.vocab_size, cfg.emb_dim)\n",
    "        self.pos_emb = nn.Embedding(cfg.context_length, cfg.emb_dim)\n",
    "        self.drop_emb = nn.Dropout(cfg.drop_rate)\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[\n",
    "                GPTTransformerBlock(cfg) for _ in range(cfg.n_layers)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg.emb_dim)\n",
    "        self.out_head = nn.Linear(cfg.emb_dim, cfg.vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, in_idx):\n",
    "        \n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(\n",
    "            torch.arange(seq_len, device=in_idx.device)\n",
    "        )\n",
    "        x = tok_embeds + pos_embeds\n",
    "        \n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        \n",
    "        return logits\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dce25dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.1381,  0.0077, -0.1963,  ..., -0.0222, -0.1060,  0.1717],\n",
      "         [ 0.3865, -0.8408, -0.6564,  ..., -0.5163,  0.2369, -0.3357],\n",
      "         [ 0.6989, -0.1829, -0.1631,  ...,  0.1472, -0.6504, -0.0056],\n",
      "         [-0.4290,  0.1669, -0.1258,  ...,  1.1579,  0.5303, -0.5549]],\n",
      "\n",
      "        [[ 0.1094, -0.2894, -0.1467,  ..., -0.0557,  0.2911, -0.2824],\n",
      "         [ 0.0882, -0.3552, -0.3527,  ...,  1.2930,  0.0053,  0.1898],\n",
      "         [ 0.6091,  0.4702, -0.4094,  ...,  0.7688,  0.3787, -0.1974],\n",
      "         [-0.0612, -0.0737,  0.4751,  ...,  1.2463, -0.3834,  0.0609]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT2_CONFIG_124M)\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2a338a",
   "metadata": {},
   "source": [
    "Using the numel() method, short for “number of elements,” we can collect the total\n",
    "number of parameters in the model’s parameter tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4990bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75023893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14bf7ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 124,412,160\n"
     ]
    }
   ],
   "source": [
    "total_params_gpt2 = (\n",
    "total_params - sum(p.numel()\n",
    "for p in model.out_head.parameters())\n",
    ")\n",
    "print(f\"Number of trainable parameters \"\n",
    "f\"considering weight tying: {total_params_gpt2:,}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa154abb",
   "metadata": {},
   "source": [
    "Lastly, let’s compute the memory requirements of the 163 million parameters in our\n",
    "GPTModel object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7595d007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "# Calculates the total size in bytes (assuming float32, 4 bytes per parameter)\n",
    "total_size_bytes = total_params*4\n",
    "total_size_mb = total_size_bytes/(1024*1024)\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b181f9",
   "metadata": {},
   "source": [
    "In conclusion, by calculating the memory requirements for the 163 million parameters in our GPTModel object and assuming each parameter is a 32-bit float taking up 4 bytes, we find that the total size of the model amounts to 621.83 MB, illustrating the relatively large storage capacity required to accommodate even relatively small LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c93ac5b",
   "metadata": {},
   "source": [
    "### Generating text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac6b1cd",
   "metadata": {},
   "source": [
    "> A function for the GPT model to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd2e1e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "        \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efbbbdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ede947",
   "metadata": {},
   "source": [
    "Next, we put the model into .eval() mode. This disables random components like\n",
    "dropout, which are only used during training, and use the generate_text_simple\n",
    "function on the encoded input tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d382fbc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "model.eval() #Disables dropout since we are not training the model\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor,\n",
    "    max_new_tokens=6,\n",
    "    context_size=GPT2_CONFIG_124M.context_length\n",
    ")\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7c2857",
   "metadata": {},
   "source": [
    "Using the .decode method of the tokenizer, we can convert the IDs back into text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0ad7072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Featureiman Byeswickattribute argue\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795e32e1",
   "metadata": {},
   "source": [
    "As we can see, the model generated gibberish, which is not at all like the coherent text\n",
    "Hello, I am a model ready to help. What happened? The reason the model is unable to\n",
    "produce coherent text is that we haven’t trained it yet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
